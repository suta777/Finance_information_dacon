{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10015141,"sourceType":"datasetVersion","datasetId":6166070},{"sourceId":10041682,"sourceType":"datasetVersion","datasetId":6185873},{"sourceId":10308942,"sourceType":"datasetVersion","datasetId":6381453},{"sourceId":10309070,"sourceType":"datasetVersion","datasetId":6381551},{"sourceId":10340600,"sourceType":"datasetVersion","datasetId":6403150},{"sourceId":10348730,"sourceType":"datasetVersion","datasetId":6408273},{"sourceId":10351858,"sourceType":"datasetVersion","datasetId":6410216},{"sourceId":10352242,"sourceType":"datasetVersion","datasetId":6410490}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#필요한 것들 install\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install transformers[torch] -U\n!pip install datasets\n!pip install langchain\n!pip install langchain_community\n!pip install -U langchain-huggingface\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install faiss-gpu\n!pip install langchain-teddynote\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\nimport unicodedata\n\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport fitz  # PyMuPDF\n\nimport random\nimport gc\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    pipeline,\n    BitsAndBytesConfig,\n    set_seed,\n    TrainingArguments, \n    Trainer\n)\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n\nfrom torch.nn.parallel import DataParallel\n\n# 시드 설정\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nset_seed(seed)\n\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_teddynote.retrievers import KiwiBM25Retriever\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain.embeddings.base import Embeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom sentence_transformers import CrossEncoder\nfrom datasets import Dataset\nfrom functools import partial\nfrom torch.cuda.amp import GradScaler\nfrom IPython.display import FileLink, display\nfrom huggingface_hub import login\nimport wandb\nimport subprocess\ntorch.utils.checkpoint.use_reentrant = False\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bitsandbytes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_source = [\n    \"「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》\",\n    \"「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》\",\n    \"「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》\",\n    \"「FIS 이슈&포커스」 22-2호 《재정성과관리제도》\",\n    \"국토교통부_행복주택출자\",\n    \"보건복지부_노인장기요양보험 사업운영\",\n    \"보건복지부_부모급여(영아수당) 지원\",\n    \"산업통상자원부_에너지바우처\",\n    \"중소벤처기업부_혁신창업사업화자금(융자)\"\n]\n\ntrain_source = [\n    \"「FIS 이슈 & 포커스」 22-3호 《재정융자사업》\",\n    \"「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》\",\n    \"1-1 2024 주요 재정통계 1권\",\n    \"2024 나라살림 예산개요\",\n    \"2024년도 성과계획서(총괄편)\",\n    \"고용노동부_내일배움카드(일반)\",\n    \"고용노동부_조기재취업수당\",\n    \"고용노동부_청년일자리창출지원\",\n    \"국토교통부_민간임대(융자)\",\n    \"국토교통부_소규모주택정비사업\",\n    \"국토교통부_전세임대(융자)\",\n    \"보건복지부_노인일자리 및 사회활동지원\",\n    \"보건복지부_생계급여\",\n    \"월간 나라재정 2023년 12월호\",\n    \"재정통계해설\",\n    \"중소벤처기업부_창업사업화지원\"\n]\n# 두 리스트를 합칩니다\nall_sources = test_source + train_source\n\n# 딕셔너리를 생성합니다\nh2n = {name : str(idx) for idx, name in enumerate(all_sources)}\nn2h = {str(idx) : name for idx, name in enumerate(all_sources)}\n\nfor i in range(len(all_sources)):\n    print(i,n2h[str(i)])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#선택할 옵션들, 여기를 수정하세요\nclass Opt:\n    def __init__(self):\n        self.llm_model = \"/kaggle/input/llama-3-1-8b-instruct\" #based on yours, 미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) , meta-llama/Meta-Llama-3.1-8B-Instruct\n        self.embeddings_model = 'intfloat/multilingual-e5-base' #based on yours, 미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) ,intfloat/multilingual-e5-large\n        self.rerank_model = 'cross-encoder/ms-marco-MiniLM-L-6-v2' #based on yours ,미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) \n        self.base_directory = \"/kaggle/input/rag-data\" #based on yours\n        self.output_dir = \"/kaggle/working/results\" #based on yours\n        \n        os.makedirs(self.output_dir, exist_ok=True) \n        #없으면 디렉토리 생성\n        self.hf_token = '' #based on yours\n        #Jupyter Notebook에서 작업했기 떄문에 직접 입력 \n        \n        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n        self.submission_csv_path = os.path.join(self.base_directory, \"sample_submission.csv\")\n        self.results_path = os.path.join(self.output_dir, \"submission.csv\")\n        self.chunk_size = 512\n        self.chunk_overlap = 32\n\n        self.rerank = False #rerank \n        self.train = False #Train으로 학습 여부\n        self.trained_model_path = \"/kaggle/working/finetuned_model\" \n        if self.train:\n            os.makedirs(self.trained_model_path, exist_ok=True) \n            #없으면 디렉토리 생성\n        self.peft = False # 학습된 모델 사용여부\n        self.finetuned_data='/kaggle/input/finetuned-dataset/finetuned_dataset.json'\n        self.template = \"\"\"\nProvide an answer to the question based on the following information.\nWhen responding:\n\nFocus on the core of the question.\nKeep the response concise, within 1–2 sentences.\nUse the units provided in the document.\nEnsure the answer is polite and in a complete form, including the subject of the question.\nAdditionally, the information and question will be in Korean, and the response should also be written in Korean.\n\nInformation:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\n<|eot_id|>\n\"\"\"\n# 다음 정보를 바탕으로 질문에 답하세요.\n# 이때 질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고,\n# 단위는 문서에서 나온 단위를 사용하며,\n# 질문의 주어를 포함해 완성된 존댓말로 답변해주세요.:\n\n# ### 정보: \n# {context}\n\n# ### 질문:\n# {question}\n\n# ### 답변:\n\n# <|eot_id|>\n# \"\"\"\n            #<|eot_id|>는 llama3.1 Instruct모델에서 사용한 특수 토큰으로 사용시 답변이 길어지는것을 방지함, 모델이 바뀐다면 확인해야함\nargs=Opt()\n\n# Hugging Face 로그인 , 미리 해당 모델의 사용 권환을 승인 받아야함\nhf_token = args.hf_token\nlogin(hf_token)\n\nwandb.login(key ='') \nwandb.init(project=\"dacon_project\")  # 프로젝트 이름 설정\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport psutil\nimport os\ndef get_memory_usage():\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss / (1024 ** 2)  # MB로 변환\n    return f\"현재 메모리 사용량: {memory_usage:.2f} MB\"\n    \nprint(get_memory_usage())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n    #PDF 텍스트 추출 후 chunk 단위로 나누기\n    #PDF 파일 열기\n    for i in h2n:\n        if i in file_path:\n            file_path = file_path.replace(i,h2n[i])\n            break\n            \n    doc = fitz.open(file_path)\n    text = ''\n    \n    for page in doc:\n        text += page.get_text()\n    \n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    chunk_temp = splitter.split_text(text)\n    chunks = [Document(page_content=t) for t in chunk_temp]\n    doc.close()\n    return chunks\n\ndef create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-base\"):\n    \"\"\"FAISS DB 생성\"\"\"\n    # 임베딩 모델 설정\n    model_kwargs = {'device': 'cuda'}\n    encode_kwargs = {'normalize_embeddings': True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=model_path,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n    # FAISS DB 생성 및 반환\n    db = FAISS.from_documents(chunks, embedding=embeddings)\n    return db\n    \n    \ndef normalize_string(string):\n    #유니코드 정규화\n    return unicodedata.normalize('NFC', string)\n    \ndef process_pdfs_from_dataframe(df, base_directory):\n    #PDF정보로 Retreiver생성\n    pdf_databases = {}\n    unique_paths = df['Source_path'].unique()\n    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n        normalized_path = normalize_string(path)\n        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n        \n        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n        print(f\"Processing {pdf_title}...\")\n        chunks = process_pdf(full_path,args.chunk_size,args.chunk_overlap)\n        db = create_vector_db(chunks,args.embeddings_model)\n                \n        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n        faiss_retriever = db.as_retriever()\n \n        retriever = EnsembleRetriever(\n            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n            weights=[0.5,0.5],\n            search_type=\"mmr\",\n        )\n        \n        pdf_databases[pdf_title] = {\n                'db': db,\n                'retriever': retriever\n        }\n        del chunks, db, kiwi_bm25_retriever, faiss_retriever, retriever\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n        \n    return pdf_databases\n        \ndef get_combined_docs(docs, question):\n    reranker = CrossEncoder(args.rerank_model)\n    \n    pairs = [[question, doc.page_content] for doc in docs]\n    scores = reranker.predict(pairs)\n    \n    scored_docs = list(zip(docs, scores))\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    \n    top_docs = [doc.page_content for doc, _ in scored_docs]\n    return \"\\n\\n\".join(top_docs)\n\ndef format_docs(docs):\n    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n    context = \"\"\n    print(len(docs))\n    for doc in docs:\n        context += doc.page_content\n        context += '\\n'\n    # print(context)\n    # print()\n    return context","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GPU초기화 ,환경은 유지되는대신 모든 변수들은 초기화됨\n\n#!for pid in $(lsof /dev/nvidia* | awk '{print $2}' | tail -n +2); do kill -9 $pid; done\n\n#nvida gpu 사용 프로세스 확인\nprint(get_memory_usage())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ndef save_dataset(dataset, filename):\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(dataset, f, ensure_ascii=False, indent=2)\n\ndef load_dataset(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        dataset_list = json.load(f)\n    return dataset_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if args.train:\n        \n    def prepare_finetune_data(df, pdf_databases):\n        finetune_data = []\n        for _, row in df.iterrows():\n            source = normalize_string(row['Source'])\n            question = row['Question']\n            answer = row['Answer']  # 실제 답변이 있다고 가정\n            \n            retriever = pdf_databases[source]['retriever']\n            docs = retriever.invoke(question)\n            context = get_combined_docs(docs, question)\n            \n            prompt = args.template.format(context=context, question=question)\n            finetune_data.append({\n                \"instruction\": prompt,\\\n                \"output\": answer\n            })\n        return finetune_data\n        \n    def preprocess_function(examples):\n        inputs = tokenizer(examples[\"instruction\"], truncation=True, padding=\"max_length\", max_length=512)\n        outputs = tokenizer(examples[\"output\"], truncation=True, padding=\"max_length\", max_length=512)\n        inputs[\"labels\"] = outputs[\"input_ids\"]\n        return inputs\n        \n    if args.finetuned_data :\n        print('Data 다운 완료')\n        finetune_dataset = load_dataset(args.finetuned_data)\n    else:\n        train_df = pd.read_csv(args.train_csv_path)\n        train_pdf_databases = process_pdfs_from_dataframe(train_df, args.base_directory)\n        finetune_dataset = prepare_finetune_data(train_df, train_pdf_databases)\n        save_dataset(finetune_dataset, '/kaggle/working/finetune_dataset.json')\n    print('Data 준비 완료')\n    model_name =args.llm_model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    # 8비트 양자화 설정\n    bnb_config = BitsAndBytesConfig(\n        load_in_8bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    \n    num_gpus = torch.cuda.device_count()\n    max_memory = {i: \"14GiB\" for i in range(num_gpus)}\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.llm_model,\n        quantization_config=bnb_config,\n        device_map=\"balanced\",\n        max_memory=max_memory,\n        trust_remote_code=True\n    )\n    model = prepare_model_for_kbit_training(model)\n    \n    peft_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    # LoRA 모델 생성\n    model = get_peft_model(model, peft_config)\n    print('모델 준비 완료')\n    \n    dataset = Dataset.from_pandas(pd.DataFrame(finetune_dataset))\n\n    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n    print('tokenized 완료')\n    training_args = TrainingArguments(\n        output_dir=args.trained_model_path,\n        num_train_epochs=10,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=16,\n        learning_rate=5e-5,\n        fp16=True,\n        save_steps=100,\n        logging_steps=50,\n        optim=\"paged_adamw_8bit\",\n        report_to=\"none\",  # wandb 로깅 활성화\n        remove_unused_columns=False,\n        gradient_checkpointing=True\n    )\n\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset\n    )\n    print('Train 시작')\n    trainer.train()\n\n    model.save_pretrained(args.trained_model_path)\n    tokenizer.save_pretrained(args.trained_model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(args.test_csv_path)\npdf_databases = process_pdfs_from_dataframe(df, args.base_directory)       ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_llm_pipeline():\n    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n    tokenizer.use_default_system_prompt = False\n\n    num_gpus = torch.cuda.device_count()\n    max_memory = {i: \"14GiB\" for i in range(num_gpus)}\n    #멀티 GPU할당 Kaggle T4 x 2 이기 때문에 하나에 14GB할당\n    model = AutoModelForCausalLM.from_pretrained(\n        args.llm_model,\n        quantization_config=BitsAndBytesConfig(\n                load_in_8bit=True  # 8-bit 양자화 활성화\n            ),\n        torch_dtype=\"auto\",\n        device_map=\"balanced\",\n        max_memory=max_memory,\n        trust_remote_code=True\n    )\n    if args.peft:\n        model = PeftModel.from_pretrained(model, args.trained_model_path)\n        model = model.merge_and_unload()\n        print('PEFT!')\n    text_generation_pipeline = pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        task=\"text-generation\",\n        temperature=0.2,\n        return_full_text=False,\n        max_new_tokens=128,\n    )\n\n    return HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nllm = setup_llm_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\nif args.rerank: print('rerank 적용')\nelse:           print('rerank 미적용')\n    \nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n    source = normalize_string(row['Source'])\n    question = row['Question']\n\n    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n    retriever = normalized_keys[source]['retriever']\n\n    prompt = PromptTemplate.from_template(args.template) \n\n    if args.rerank:\n        combine_with_question = partial(get_combined_docs,question=question)\n    else:\n        combine_with_question = format_docs\n        \n    rag_chain = (\n        {\"context\": retriever | combine_with_question, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    print(f\"Question: {question}\")\n    full_response = rag_chain.invoke(question)\n    print(f\"Answer: {full_response}\\n\")\n        \n    # 결과 저장\n    results.append({\n        \"Source\": row['Source'],\n        \"Source_path\": row['Source_path'],\n        \"Question\": question,\n        \"Answer\": full_response\n\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 제출용 샘플 파일 로드\nsubmit_df = pd.read_csv(args.submission_csv_path)\n# 생성된 답변에서 앞뒤의 공백 및 줄바꿈 제거 후 제출 DataFrame에 추가\nsubmit_df['Answer'] = [item['Answer'].strip() for item in results]  # strip()으로 앞뒤 공백 제거\nsubmit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")  # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [주의]\n\nprint(submit_df.head(3))\nsubmit_df.to_csv(args.results_path, encoding='UTF-8-sig', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#결과 다운 코드 생성\ndef download_file(path):\n    os.chdir(os.path.dirname(path))\n    \n    command = f\"zip -r output.zip {os.path.basename(path)}/\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink('output.zip'))\n    \n#download_file(args.output_dir)\ndownload_file(args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}