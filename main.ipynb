{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10015141,"sourceType":"datasetVersion","datasetId":6166070}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#필요한 것들 install\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install transformers[torch] -U\n!pip install datasets\n!pip install langchain\n!pip install langchain_community\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install faiss-gpu\n!pip install langchain-teddynote\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport unicodedata\n\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport fitz  # PyMuPDF\n\nimport random\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    pipeline,\n    BitsAndBytesConfig,\n    set_seed\n)\n\n# 시드 설정\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nset_seed(seed)\n\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\n\nimport subprocess\nfrom IPython.display import FileLink, display\nfrom huggingface_hub import login\nimport wandb\n\ntorch.utils.checkpoint.use_reentrant = False\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bitsandbytes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_source = [\n    \"「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》\",\n    \"「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》\",\n    \"「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》\",\n    \"「FIS 이슈&포커스」 22-2호 《재정성과관리제도》\",\n    \"국토교통부_행복주택출자\",\n    \"보건복지부_노인장기요양보험 사업운영\",\n    \"보건복지부_부모급여(영아수당) 지원\",\n    \"산업통상자원부_에너지바우처\",\n    \"중소벤처기업부_혁신창업사업화자금(융자)\"\n]\n\ntrain_source = [\n    \"「FIS 이슈 & 포커스」 22-3호 《재정융자사업》\",\n    \"「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》\",\n    \"1-1 2024 주요 재정통계 1권\",\n    \"2024 나라살림 예산개요\",\n    \"2024년도 성과계획서(총괄편)\",\n    \"고용노동부_내일배움카드(일반)\",\n    \"고용노동부_조기재취업수당\",\n    \"고용노동부_청년일자리창출지원\",\n    \"국토교통부_민간임대(융자)\",\n    \"국토교통부_소규모주택정비사업\",\n    \"국토교통부_전세임대(융자)\",\n    \"보건복지부_노인일자리 및 사회활동지원\",\n    \"보건복지부_생계급여\",\n    \"월간 나라재정 2023년 12월호\",\n    \"재정통계해설\",\n    \"중소벤처기업부_창업사업화지원\"\n]\n# 두 리스트를 합칩니다\nall_sources = test_source + train_source\n\n# 딕셔너리를 생성합니다\nh2n = {name : str(idx) for idx, name in enumerate(all_sources)}\nn2h = {str(idx) : name for idx, name in enumerate(all_sources)}\n\nfor i in range(len(all_sources)):\n    print(i,n2h[str(i)])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#선택할 옵션들, 여기를 수정하세요\nclass Opt:\n    def __init__(self):\n        self.llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" #based on yours\n        self.embeddings_model = 'intfloat/multilingual-e5-large' #based on yours\n        self.base_directory = \"/kaggle/input/rag-data\" #based on yours\n        self.output_dir = \"/kaggle/working/results\" #based on yours\n        os.makedirs(self.output_dir, exist_ok=True) \n        #없으면 디렉토리 생성\n        self.hf_token = '' #based on yours\n        #Jupyter Notebook에서 작업했기 떄문에 직접 입력 \n        \n        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n        self.submission_csv_path = os.path.join(self.base_directory, \"sample_submission.csv\")\n        self.results_path = os.path.join(self.output_dir, \"submission.csv\")\n        self.chunk_size = 512\n        self.chunk_overlap = 32\n\n        self.template = \"\"\"\n            다음 정보를 바탕으로 질문에 답하세요:\n            {context}\n            \n            ### 질문:\n            {question}\n            \n            ### 답변:\n            \n            <|eot_id|>\n            \"\"\"\n            #<|eot_id|>는 llama3.1 Instruct모델에서 사용한 특수 토큰으로 사용시 답변이 길어지는것을 방지함, 모델이 바뀐다면 확인해야함\nargs=Opt()\n\n# Hugging Face 로그인 , 미리 해당 모델의 사용 권환을 승인 받아야함\nhf_token = args.hf_token\nlogin(hf_token)\n\n# wandb.login(key ='') \n# wandb.init(project=\"\")  # 프로젝트 이름 설정\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n    #PDF 텍스트 추출 후 chunk 단위로 나누기\n    #PDF 파일 열기\n    for i in h2n:\n        if i in file_path:\n            file_path = file_path.replace(i,h2n[i])\n            break\n            \n    doc = fitz.open(file_path)\n    text = ''\n    \n    for page in doc:\n        text += page.get_text()\n    \n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    chunk_temp = splitter.split_text(text)\n    chunks = [Document(page_content=t) for t in chunk_temp]\n    return chunks\n\ndef create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-large\"):\n    #FAISS DB생성\n    model_kwargs = {'device': 'cuda'}\n    encode_kwargs = {'normalize_embeddings': True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=model_path,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n    db = FAISS.from_documents(chunks, embedding=embeddings)\n    return db\n    \ndef normalize_string(string):\n    #유니코드 정규화\n    return unicodedata.normalize('NFC', string)\n\ndef process_pdfs_from_dataframe(df, base_directory):\n    #PDF정보로 Retreiver생성\n    all_chunks = []\n    unique_paths = df['Source_path'].unique()\n    \n    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n        normalized_path = normalize_string(path)\n        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n        \n        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n        print(f\"Processing {pdf_title}...\")\n        \n        chunks = process_pdf(full_path,args.chunk_size,args.chunk_overlap)\n        all_chunks.extend(chunks)\n    \n    print(\"Creating vector database...\")\n    db = create_vector_db(all_chunks,args.embeddings_model)\n    \n    retriever = db.as_retriever()\n    \n    return {\n        'db': db,\n        'retriever': retriever\n    }\n    \ndef get_combined_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(args.train_csv_path)\npdf_databases = process_pdfs_from_dataframe(df, args.base_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_llm_pipeline():\n    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n    tokenizer.use_default_system_prompt = False\n\n    num_gpus = torch.cuda.device_count()\n    max_memory = {i: \"14GiB\" for i in range(num_gpus)}\n    #멀티 GPU할당 Kaggle T4 x 2 이기 때문에 하나에 14GB할당\n    model = AutoModelForCausalLM.from_pretrained(\n        args.llm_model,\n        quantization_config=BitsAndBytesConfig(\n                load_in_8bit=True  # 8-bit 양자화 활성화\n            ),\n        torch_dtype=\"auto\",\n        device_map=\"balanced\",\n        max_memory=max_memory,\n        trust_remote_code=True\n    )\n    text_generation_pipeline = pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        task=\"text-generation\",\n        return_full_text=False,\n        max_new_tokens=256,\n    )\n\n    return HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nllm = setup_llm_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\n\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n    source = normalize_string(row['Source'])\n    question = row['Question']\n\n    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n    retriever = normalized_keys['retriever']\n\n    prompt = PromptTemplate.from_template(args.template) \n   \n    # RAG 체인 정의\n    rag_chain = (\n        {\"context\": retriever | get_combined_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    print(f\"Question: {question}\")\n    full_response = rag_chain.invoke(question)\n    print(f\"Answer: {full_response}\\n\")\n\n    # 결과 저장\n    results.append({\n        \"Source\": row['Source'],\n        \"Source_path\": row['Source_path'],\n        \"Question\": question,\n        \"Answer\": full_response\n\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#임시\nimport pandas as pd\nimport os\nimport subprocess\nfrom IPython.display import FileLink, display, HTML\nclass Opt():\n    def __init__(self):\n        self.results_path='/kaggle/working/results/submission.csv'\n        self.submission_csv_path = '/kaggle/input/rag-data/sample_submission.csv'\n        self.output_dir = '/kaggle/working/results'\n        os.makedirs(self.output_dir, exist_ok=True) \n\nargs=Opt()\nresults = []\nfor _ in range(98):    # 결과 저장\n    results.append({\n        \"Source\": 'a',\n        \"Source_path\": 'a',\n        \"Question\": 'a',\n        \"Answer\": 'a'\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 제출용 샘플 파일 로드\nsubmit_df = pd.read_csv(args.submission_csv_path)\n\n# 생성된 답변에서 앞뒤의 공백 및 줄바꿈 제거 후 제출 DataFrame에 추가\nsubmit_df['Answer'] = [item['Answer'].strip() for item in results]  # strip()으로 앞뒤 공백 제거\nsubmit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")  # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [주의]\n\nprint(submit_df.head(3))\nsubmit_df.to_csv(args.results_path, encoding='UTF-8-sig', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#결과 다운 코드 생성\ndef download_file(path):\n    os.chdir(os.path.dirname(path))\n    \n    command = f\"zip -r output.zip {os.path.basename(path)}/\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink('output.zip'))\n    \ndownload_file(args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}