{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10015141,"sourceType":"datasetVersion","datasetId":6166070},{"sourceId":10308942,"sourceType":"datasetVersion","datasetId":6381453}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#필요한 것들 install\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install transformers[torch] -U\n!pip install datasets\n!pip install langchain\n!pip install langchain_community\n!pip install -U langchain-huggingface\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install faiss-gpu\n!pip install langchain-teddynote\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nimport unicodedata\n\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport fitz  # PyMuPDF\n\nimport random\nimport gc\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    pipeline,\n    BitsAndBytesConfig,\n    set_seed\n)\nfrom torch.nn.parallel import DataParallel\n\n# 시드 설정\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nset_seed(seed)\n\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_teddynote.retrievers import KiwiBM25Retriever\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain.embeddings.base import Embeddings\nfrom sentence_transformers import CrossEncoder\n\nimport subprocess\nfrom IPython.display import FileLink, display\nfrom huggingface_hub import login\nimport wandb\n\ntorch.utils.checkpoint.use_reentrant = False\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bitsandbytes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_source = [\n    \"「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》\",\n    \"「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》\",\n    \"「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》\",\n    \"「FIS 이슈&포커스」 22-2호 《재정성과관리제도》\",\n    \"국토교통부_행복주택출자\",\n    \"보건복지부_노인장기요양보험 사업운영\",\n    \"보건복지부_부모급여(영아수당) 지원\",\n    \"산업통상자원부_에너지바우처\",\n    \"중소벤처기업부_혁신창업사업화자금(융자)\"\n]\n\ntrain_source = [\n    \"「FIS 이슈 & 포커스」 22-3호 《재정융자사업》\",\n    \"「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》\",\n    \"1-1 2024 주요 재정통계 1권\",\n    \"2024 나라살림 예산개요\",\n    \"2024년도 성과계획서(총괄편)\",\n    \"고용노동부_내일배움카드(일반)\",\n    \"고용노동부_조기재취업수당\",\n    \"고용노동부_청년일자리창출지원\",\n    \"국토교통부_민간임대(융자)\",\n    \"국토교통부_소규모주택정비사업\",\n    \"국토교통부_전세임대(융자)\",\n    \"보건복지부_노인일자리 및 사회활동지원\",\n    \"보건복지부_생계급여\",\n    \"월간 나라재정 2023년 12월호\",\n    \"재정통계해설\",\n    \"중소벤처기업부_창업사업화지원\"\n]\n# 두 리스트를 합칩니다\nall_sources = test_source + train_source\n\n# 딕셔너리를 생성합니다\nh2n = {name : str(idx) for idx, name in enumerate(all_sources)}\nn2h = {str(idx) : name for idx, name in enumerate(all_sources)}\n\nfor i in range(len(all_sources)):\n    print(i,n2h[str(i)])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#선택할 옵션들, 여기를 수정하세요\nclass Opt:\n    def __init__(self):\n        self.llm_model = \"/kaggle/input/llama-3-1-8b-instruct\" #based on yours, 미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) , meta-llama/Meta-Llama-3.1-8B-Instruct\n        self.embeddings_model = '/kaggle/input/multilingual-e5-base' #based on yours, 미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) ,intfloat/multilingual-e5-large\n        self.rerank_model = 'cross-encoder/ms-marco-MiniLM-L-6-v2' #based on yours ,미리 다운받고 로컬 주소를 입력해도 됨(다운 시간 절약) \n        self.base_directory = \"/kaggle/input/rag-data\" #based on yours\n        self.output_dir = \"/kaggle/workindg/results\" #based on yours\n        os.makedirs(self.output_dir, exist_ok=True) \n        #없으면 디렉토리 생성\n        self.hf_token = '' #based on yours\n        #Jupyter Notebook에서 작업했기 떄문에 직접 입력 \n        \n        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n        self.submission_csv_path = os.path.join(self.base_directory, \"sample_submission.csv\")\n        self.results_path = os.path.join(self.output_dir, \"submission.csv\")\n        self.chunk_size = 512\n        self.chunk_overlap = 32\n\n        self.template = \"\"\"\n            다음 정보를 바탕으로 질문에 답하세요:\n            {context}\n            \n            ### 질문:\n            {question}\n            \n            ### 답변:\n            \n            <|eot_id|>\n            \"\"\"\n            #<|eot_id|>는 llama3.1 Instruct모델에서 사용한 특수 토큰으로 사용시 답변이 길어지는것을 방지함, 모델이 바뀐다면 확인해야함\nargs=Opt()\n\n# Hugging Face 로그인 , 미리 해당 모델의 사용 권환을 승인 받아야함\nhf_token = args.hf_token\nlogin(hf_token)\n\n# wandb.login(key ='') \n# wandb.init(project=\"\")  # 프로젝트 이름 설정\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport psutil\nimport os\ndef get_memory_usage():\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss / (1024 ** 2)  # MB로 변환\n    return f\"현재 메모리 사용량: {memory_usage:.2f} MB\"\n    \nprint(get_memory_usage())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n    #PDF 텍스트 추출 후 chunk 단위로 나누기\n    #PDF 파일 열기\n    for i in h2n:\n        if i in file_path:\n            file_path = file_path.replace(i,h2n[i])\n            break\n            \n    doc = fitz.open(file_path)\n    text = ''\n    \n    for page in doc:\n        text += page.get_text()\n    \n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    chunk_temp = splitter.split_text(text)\n    chunks = [Document(page_content=t) for t in chunk_temp]\n    doc.close()\n    return chunks\n\nclass MultiGPUHuggingFaceEmbeddings(Embeddings):\n    def __init__(self, model_path, devices=None):\n        self.devices = devices if devices else [f\"cuda:{i}\" for i in range(torch.cuda.device_count())]\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModel.from_pretrained(model_path)\n\n        if len(self.devices) > 1:\n            self.model = torch.nn.DataParallel(self.model, device_ids=[int(d.split(\":\")[-1]) for d in self.devices])\n        self.model.to(self.devices[0])\n\n    def embed_documents(self, texts):\n        return [self._embed_text(text) for text in texts]\n\n    def embed_query(self, text):\n        return self._embed_text(text)\n\n    def _embed_text(self, text):\n        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.devices[0])\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n\ndef create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-base\"):\n    # GPU 확인 및 멀티 GPU 설정\n    devices = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [\"cpu\"]\n    print(f\"Using devices: {devices}\")\n\n    # 사용자 정의 멀티 GPU 임베딩 클래스 사용\n    embeddings = MultiGPUHuggingFaceEmbeddings(model_path, devices=devices)\n    \n    # FAISS 데이터베이스 생성\n    db = FAISS.from_documents(chunks, embedding=embeddings)\n    del embeddings\n    return db\n\n    \ndef normalize_string(string):\n    #유니코드 정규화\n    return unicodedata.normalize('NFC', string)\n    \ndef process_pdfs_from_dataframe(df, base_directory):\n    #PDF정보로 Retreiver생성\n    pdf_databases = {}\n    unique_paths = df['Source_path'].unique()\n    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n    \n        normalized_path = normalize_string(path)\n        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n        \n        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n        print(f\"Processing {pdf_title}...\")\n        chunks = process_pdf(full_path,args.chunk_size,args.chunk_overlap)\n        db = create_vector_db(chunks,args.embeddings_model)\n                \n        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n        faiss_retriever = db.as_retriever()\n \n        retriever = EnsembleRetriever(\n            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n            weights=[0.5,0.5],\n            search_type=\"mmr\",\n        )\n        \n        pdf_databases[pdf_title] = {\n                'db': db,\n                'retriever': retriever\n        }\n        del chunks, db, kiwi_bm25_retriever, faiss_retriever, retriever\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n        \n    return pdf_databases\n        \ndef get_combined_docs(docs, question):\n    reranker = CrossEncoder(args.rerank_model)\n    \n    pairs = [[question, doc.page_content] for doc in docs]\n    scores = reranker.predict(pairs)\n    \n    scored_docs = list(zip(docs, scores))\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    \n    top_docs = [doc.page_content for doc, _ in scored_docs[:3]]\n    return \"\\n\\n\".join(top_docs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GPU초기화 ,환경은 유지되는대신 모든 변수들은 초기화됨\n\n#!for pid in $(lsof /dev/nvidia* | awk '{print $2}' | tail -n +2); do kill -9 $pid; done\n\n#nvida gpu 사용 프로세스 확인\nprint(get_memory_usage())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(args.test_csv_path)\npdf_databases = process_pdfs_from_dataframe(df, args.base_directory)       ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_llm_pipeline():\n    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n    tokenizer.use_default_system_prompt = False\n\n    num_gpus = torch.cuda.device_count()\n    max_memory = {i: \"14GiB\" for i in range(num_gpus)}\n    #멀티 GPU할당 Kaggle T4 x 2 이기 때문에 하나에 14GB할당\n    model = AutoModelForCausalLM.from_pretrained(\n        args.llm_model,\n        quantization_config=BitsAndBytesConfig(\n                load_in_8bit=True  # 8-bit 양자화 활성화\n            ),\n        torch_dtype=\"auto\",\n        device_map=\"balanced\",\n        max_memory=max_memory,\n        trust_remote_code=True\n    )\n    text_generation_pipeline = pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        task=\"text-generation\",\n        return_full_text=False,\n        max_new_tokens=256,\n    )\n\n    return HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nllm = setup_llm_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\n\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n    source = normalize_string(row['Source'])\n    question = row['Question']\n\n    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n    retriever = normalized_keys[source]['retriever']\n\n    prompt = PromptTemplate.from_template(args.template) \n\n    docs = retriever.get_relevant_documents(question)\n    context = get_combined_docs(docs,question)\n    \n    # RAG 체인 정의\n    rag_chain = (\n        {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    print(f\"Question: {question}\")\n    full_response = rag_chain.invoke({'context':context,'qeustion': question})\n    print(f\"Answer: {full_response}\\n\")\n\n    # 결과 저장\n    results.append({\n        \"Source\": row['Source'],\n        \"Source_path\": row['Source_path'],\n        \"Question\": question,\n        \"Answer\": full_response\n\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 제출용 샘플 파일 로드\nsubmit_df = pd.read_csv(args.submission_csv_path)\n# 생성된 답변에서 앞뒤의 공백 및 줄바꿈 제거 후 제출 DataFrame에 추가\nsubmit_df['Answer'] = [item['Answer'].strip() for item in results]  # strip()으로 앞뒤 공백 제거\nsubmit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")  # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [주의]\n\nprint(submit_df.head(3))\nsubmit_df.to_csv(args.results_path, encoding='UTF-8-sig', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#결과 다운 코드 생성\ndef download_file(path):\n    os.chdir(os.path.dirname(path))\n    \n    command = f\"zip -r output.zip {os.path.basename(path)}/\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink('output.zip'))\n    \ndownload_file(args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}